{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "#from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "# moves your model to train on your gpu if available else it uses your cpu\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger des données MNIST à partir de Torchvision\n",
    "Torchvision permet de charger les données du MNIST de manière pratique. Nous utiliserons un batch_size de 64 pour la formation et de 1000 pour les tests sur cet ensemble de données. Les valeurs 0,1307 et 0,3081 utilisées pour la transformation Normalize() ci-dessous sont la moyenne globale et l'écart-type de l'ensemble de données MNIST. **transforme.ToTensor()** convertit l'ensemble du tableau en tenseur de torche et le divise par 255.  Les valeurs sont donc comprises entre 0,0 et 1,0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On defini la transformation qui normalise les données\n",
    "transform = transforms.Compose([ transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,),(0.3081,))\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to DATA_MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b35a9958364301b375f8270ad80698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DATA_MNIST/MNIST/raw/train-images-idx3-ubyte.gz to DATA_MNIST/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to DATA_MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1e0f6167a34020ae98e895b7751889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DATA_MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to DATA_MNIST/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to DATA_MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a567cb691c7b4840b400f27cbdc8a221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DATA_MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to DATA_MNIST/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to DATA_MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b113beaba84b3eb151b423c4c2832a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting DATA_MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to DATA_MNIST/MNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vw12/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1595629430416/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# On télécharge et charge les données \n",
    "train_set = datasets.MNIST('DATA_MNIST/', download=True, train=True, transform=transform)\n",
    "trainLoader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "test_set = datasets.MNIST('DATA_MNIST/', download=True, train=False, transform=transform)\n",
    "testLoader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 60000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set),len(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessing des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b032830>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 64\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour des expériences répétable il est important de fixer la graine\n",
    "Il faut souligner que **cudnn** n'utilise que les algorithmes non déterministes qui peuvent est désactivés **$torch.backends.cudnn.enabled = False$**\n",
    "\n",
    "Dans PyTorch, **DataLoader** contient quelques options interessantes autres que dataset et batch size. par exemple l'utilisation de $num workers >1$ pour utiliser les sous processus pour charger les données de manière asynchrone ou utiliser la RAM (via **pin_memory**) pour accelérer les transfert de RAM vers le GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le type de données:  <class 'torch.Tensor'>\n",
      "la tailles des images : torch.Size([64, 1, 28, 28])\n",
      "la taille des labels : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(testLoader)\n",
    "batch_idx, (images, labels) = next(examples)\n",
    "print(\"le type de données: \",type(images)) # Checking the datatype \n",
    "print(\"la tailles des images :\", images.shape) # the size of the image\n",
    "print(\"la taille des labels :\", labels.shape) # the size of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformons notre objet trainloader en un itérateur avec iter afin que nous puissions accéder à nos images et à nos étiquettes à partir de ce générateur. Nous pouvons voir la forme comme $64\\times 1\\times 28\\times 28$. cela signifi que:\n",
    "\n",
    "- 64: Represents 64 images\n",
    "- 1 : un cannal donc la couleur ==>> Grayscale(grise)\n",
    "- $28$ par 28 pixel: la forme des images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(images[i][0], cmap='gray')\n",
    "    plt.title(\"label: {}\".format(labels[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construction du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Nous allons constuire notre réseau en utilisant la méthode **nn.module** dans la classe des objets orientées. Ce réseau sera composé de 2 couches de convolutions 2D suivies de des couches FC. Comme fonctions d'activations on utilisera la ReLU et comme moyen de regularisation  on utilisera le Dropout\n",
    "\n",
    "La fonction **Forward()** définie la façon dont on calcule la sortie en utilisant les couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # couche du réseau de convolution\n",
    "        self.convolutaional_neural_network_layers = nn.Sequential(\n",
    "                # ici on defini notre couche de covolution 2D\n",
    "                # On peut calculer la taille de sortie de chaque de convolution comme:\n",
    "                # outpout = [(in_channel + 2*padding - kernel_size) / stride] + 1\n",
    "                # On a in_channels=1 parce que nos images sont en noire et blanc\n",
    "                nn.Conv2d(in_channels=1, out_channels=12, kernel_size=3, padding=1, stride=1),#(N, 1, 28, 28) \n",
    "                nn.ReLU(),\n",
    "                # après la première couche de convolution la sortie est\n",
    "                # [(28 + 2*1 - 3)/1] + 1 = 28. \n",
    "                nn.MaxPool2d(kernel_size=2), \n",
    "                # Puisqu'on appliqué le max pooling avec kernel=2, on a divisé par 2, donc on obtient\n",
    "                # 28 / 2 = 14\n",
    "          \n",
    "                # la sortie de la seconde couche\n",
    "                nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, padding=1, stride=1),\n",
    "                nn.ReLU(),\n",
    "                # la taille de sortie est :\n",
    "                # [(14 + 2*1 - 3)/1] + 1 = 14. \n",
    "                nn.MaxPool2d(kernel_size=2) \n",
    "                # Puisqu'on appliqué le max pooling avec kernel=2, on a divisé par 2, donc on obtient\n",
    "                # 14 / 2 = 7\n",
    "        )\n",
    "\n",
    "        # les couches linéaires\n",
    "        self.linear_layers = nn.Sequential(\n",
    "                # On a output_channel=24 de la seconde couche, et 7*7 est derivée par la formule\n",
    "                # qui est la sortie de chaque couche\n",
    "                nn.Linear(in_features=24*7*7, out_features=64),          \n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.2), \n",
    "                nn.Linear(in_features=64, out_features=10) # on a 10 classes\n",
    "        )\n",
    "        \n",
    "    # Définition de la passe avant \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutaional_neural_network_layers(x)\n",
    "        # Après avoir obtenu la sortie de notre couche convolutionnelle,\n",
    "        # nous devons l'aplatir ou réorganiser la sortie en un vecteur        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        # puis ça passe à travers les couches linéaires\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction forward() pass (la passe en avant) définit la façon dont notre sortie est calculée. La ligne x.view(x.size(0), -1) aplatit la sortie de la couche de convolution en un vecteur. La plupart du temps, lors de l'expérimentation de modèles plus complexes, il est conseillé d'afficher les valeurs du tenseur pour faciliter le débogage.\n",
    "\n",
    "Définissons les optimiseurs que nous utiliserons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (convolutaional_neural_network_layers): Sequential(\n",
       "    (0): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_layers): Sequential(\n",
       "    (0): Linear(in_features=1176, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après la passe en avant, la loss function est calculée à partir de la **sortie souhaitée( etiquette)** et les prédictions  afin de mettre à jour les poids. La mise en place de la fonction de perte est une étape assez simple. On utilise ici la perte d'entropie croisée, qui mesure la performance d'un modèle de classification dont la sortie est une valeur de probabilité comprise entre 0 et 1. Il convient de noter que la perte d'entropie croisée augmente lorsque la probabilité prévue diverge de l'étiquette réelle.\n",
    "\n",
    "Ensuite, nous utiliserons l'optimiseur Stochastic Gradient Descent pour la mise à jour des hyperparamètres model.parameters() fournira les paramètres pouvant être appris à l'optimiseur et lr=0.01 définit les pas d'apprentissage pour les mises à jour des paramètres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de noter que lorsqu'on utilise le GPU pour entrainer le réseau, il faudra envoyer les paramètres du modèles au GPU en utilisant **model.cuda**. C'est important de transférer les paramètres aux device appropriés avant de les passer à l'optimiseur, sinon l'optimiseur ne sera pas capable de les suivrent \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage et test du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le chargement des données est géré par Dataloader, ensuite on commence à mettre à zero le gradient en utilisant **optimizer.zero_grad()** puisque Pytorch par defaut cummule le gradient\n",
    "le **backward()** permet de collecter le nouveau  gradient qu'on propagera dans les paramètres du réseau via à **optimizer.step()**\n",
    "\n",
    "Notre modèle est maintenant prêt à être entrainé. On commençe par établir une taille d'époque. Une époque c'est, lorsqu'on traverse toute la base de données une seule fois pendant l'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "train_loss, val_loss = [], []\n",
    "accuracy_total_train, accuracy_total_val = [], []\n",
    "\n",
    "def Training(epochs):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total = 0\n",
    "        # training our model\n",
    "        for idx, (image, label) in enumerate(trainLoader):\n",
    "            # cela est nécessaire lorsqu'on travaille sur un GPU\n",
    "            #image, label = image.to(device), label.to(device)\n",
    "            # mettre le gradient à zero\n",
    "            optimizer.zero_grad()\n",
    "            output= model(image)\n",
    "\n",
    "            loss = F.nll_loss(output, label)\n",
    "            total_train_loss += loss.item()\n",
    "            # retropropager la loss dans le réseau\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred = F.softmax(output, dim=1)\n",
    "            for i, p in enumerate(pred):\n",
    "                if label[i] == torch.max(p.data, 0)[1]:\n",
    "                    total = total + 1\n",
    "\n",
    "        accuracy_train = total / len(train_set)\n",
    "        accuracy_total_train.append(accuracy_train)\n",
    "\n",
    "        total_train_loss = total_train_loss / (idx + 1)\n",
    "        train_loss.append(total_train_loss)\n",
    "\n",
    "        # test de notre modèle\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        for idx, (image, label) in enumerate(testLoader):\n",
    "            pred = model(image)\n",
    "            loss = F.nll_loss(pred, label)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "            for i, p in enumerate(pred):\n",
    "                if label[i] == torch.max(p.data, 0)[1]:\n",
    "                    total = total + 1\n",
    "\n",
    "        accuracy_val = total / len(test_set)\n",
    "        accuracy_total_val.append(accuracy_val)\n",
    "\n",
    "        total_val_loss = total_val_loss / (idx + 1)\n",
    "        val_loss.append(total_val_loss)\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Epoch: {}/{}  \".format(epoch, epochs),\n",
    "                \"Training loss: {:.4f}  \".format(total_train_loss),\n",
    "                \"Testing loss: {:.4f}  \".format(total_val_loss),\n",
    "                \"Train accuracy: {:.4f}  \".format(accuracy_train),\n",
    "                \"Test accuracy: {:.4f}  \".format(accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/3   Training loss: nan   Testing loss: nan   Train accuracy: 0.0987   Test accuracy: 0.0980  \n",
      "Epoch: 2/3   Training loss: nan   Testing loss: nan   Train accuracy: 0.0987   Test accuracy: 0.0980  \n"
     ]
    }
   ],
   "source": [
    "Training(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
